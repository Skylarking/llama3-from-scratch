{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLAMA3文本生成\n",
    "- 同样的代码可以看[example_text_completion.py](example_text_completion.py)，py文件可以调试"
   ],
   "id": "9f1ab76924525f42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ①设置工作目录和环境变量",
   "id": "2683bc7be2577390"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T06:30:53.830388Z",
     "start_time": "2024-07-24T06:30:53.825474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# 获取当前工作目录\n",
    "current_directory = os.getcwd()\n",
    "print(\"当前工作目录:\", current_directory)\n",
    "\n",
    "# 设置新的工作目录\n",
    "new_directory = \"/mnt/d/code/llama3-from-scratch\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# 再次获取当前工作目录，确认是否更改成功\n",
    "current_directory = os.getcwd()\n",
    "print(\"新的工作目录:\", current_directory)\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '5678'"
   ],
   "id": "bd18005c39647576",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: /home/liangxianbing\n",
      "新的工作目录: /mnt/d/code/llama3-from-scratch\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ②构建llama3模型\n",
   "id": "21f9d7004eb488c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T06:31:03.787828Z",
     "start_time": "2024-07-24T06:31:02.416326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Optional\n",
    "# import fire\n",
    "from llama import Dialog, Llama"
   ],
   "id": "a0951ca904d03f8d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T06:31:08.538074Z",
     "start_time": "2024-07-24T06:31:08.535105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt_dir = 'Meta-Llama-3-8B/original'\n",
    "tokenizer_path = 'Meta-Llama-3-8B/original/tokenizer.model'\n",
    "max_seq_len = 512       # 输入的最大token数（包含prompt的token）\n",
    "max_batch_size = 6      # 最大batch size，即最大同时生成几个句子（prompts数决定）"
   ],
   "id": "df3a20b80da8efa2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T06:32:36.559800Z",
     "start_time": "2024-07-24T06:31:11.190500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构建模型\n",
    "generator = Llama.build(\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=max_batch_size,\n",
    "    )\n",
    "print('model built done!')"
   ],
   "id": "50c8468e71ec1905",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangxianbing/miniconda3/envs/py312/lib/python3.12/site-packages/torch/__init__.py:749: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 84.79 seconds\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ③构建prompts",
   "id": "6a62694ad84fb33f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T06:36:09.536064Z",
     "start_time": "2024-07-24T06:36:09.533056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts: List[str] = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "        \"I believe the meaning of life is\",\n",
    "        \"Simply put, the theory of relativity states that \",\n",
    "        \"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "        Hi everyone,\n",
    "\n",
    "        I just \"\"\",\n",
    "        # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "        \"\"\"Translate English to French:\n",
    "\n",
    "        sea otter => loutre de mer\n",
    "        peppermint => menthe poivrée\n",
    "        plush girafe => girafe peluche\n",
    "        cheese =>\"\"\",\n",
    "    ]"
   ],
   "id": "855b8df40d588b03",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ④文本生成\n",
    "- temperature用法：logits / temperature，然后再softmax，会改变分布的平滑性；较大的temperature会让分布更平滑，会更有多样性；较小会更尖锐，也会更确定\n",
    "    ```\n",
    "    if temperature > 0:\n",
    "        probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "    ```\n",
    "- top_p用法：越大越有多样性，越小越确定\n",
    "  1. logits过softmax后得到probs，然后对probs排序;\n",
    "  2. 然后求每个位置的累加;\n",
    "  3. 生成mask：累加-probs > p的位置为1\n",
    "  4. mask为1的位置置为0\n",
    "  5. 重新计算概率（因为某些置为0了）\n",
    "  6. 做概率采样，采样一次得到token\n",
    "    ```\n",
    "    def sample_top_p(probs, p):\n",
    "        \"\"\"\n",
    "        Perform top-p (nucleus) sampling on a probability distribution.\n",
    "    \n",
    "        Args:\n",
    "            probs (torch.Tensor): Probability distribution tensor.\n",
    "            p (float): Probability threshold for top-p sampling.\n",
    "    \n",
    "        Returns:\n",
    "            torch.Tensor: Sampled token indices.\n",
    "    \n",
    "        Note:\n",
    "            Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
    "            exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
    "        \"\"\"\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "        mask = probs_sum - probs_sort > p\n",
    "        probs_sort[mask] = 0.0\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        next_token = torch.gather(probs_idx, -1, next_token)    # 取token真正的位置（因为sort了）\n",
    "        return next_token\n",
    "    ```"
   ],
   "id": "81c16c3b7d7ba3f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T06:45:05.087052Z",
     "start_time": "2024-07-24T06:36:32.290699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_gen_len = 64    # 生成的token数\n",
    "temperature = 0.6   # 用法：logits / temperature，然后再softmax，会改变分布的平滑性；较大的temperature会让分布更平滑，会更有多样性；较小会更尖锐，也会更确定\n",
    "top_p = 0.6 # 用法：较大时，考虑更多词，更有多样性；较小时，只会考虑概率足够大的，更加具有确定性\n",
    "results = generator.text_completion(\n",
    "        prompts,\n",
    "        max_gen_len=max_gen_len,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "print('text generation done!')"
   ],
   "id": "1f263107c65e0f42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text generation done!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:13:41.059946Z",
     "start_time": "2024-07-24T07:13:41.056806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印出来\n",
    "for prompt, result in zip(prompts, results):\n",
    "    print(prompt)\n",
    "    print(f\"> {result['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ],
   "id": "6b8eb6f8bad32262",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy. I believe that we are all born with a purpose, and that we have the ability to choose our own path. I believe that we are all here to learn and grow, and that we should never stop learning. I believe that we are all connected, and that we should always be kind and compassionate\n",
      "\n",
      "==================================\n",
      "\n",
      "Simply put, the theory of relativity states that \n",
      "> 1) the laws of physics are the same for all non-accelerating observers and 2) the speed of light in a vacuum is the same for all observers, regardless of the motion of the light source. The first part of the theory is known as the principle of relativity. The second part is known\n",
      "\n",
      "==================================\n",
      "\n",
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "        Hi everyone,\n",
      "\n",
      "        I just \n",
      ">  wanted to take a moment to congratulate you all on the launch of the\n",
      "        new website.  It's a great accomplishment and I'm sure it will be a great\n",
      "        resource for the community.  Keep up the good work!\n",
      "\n",
      "        Best regards,\n",
      "        [Your Name]\n",
      "\n",
      "A more detailed message that includes a\n",
      "\n",
      "==================================\n",
      "\n",
      "Translate English to French:\n",
      "\n",
      "        sea otter => loutre de mer\n",
      "        peppermint => menthe poivrée\n",
      "        plush girafe => girafe peluche\n",
      "        cheese =>\n",
      ">  fromage\n",
      "        penguin => manchot\n",
      "        fish => poisson\n",
      "        turtle => tortue\n",
      "        dog => chien\n",
      "        cat => chat\n",
      "        bear => ours\n",
      "        monkey => singe\n",
      "        rabbit => lapin\n",
      "        fox => renard\n",
      "        lion => lion\n",
      "        elephant\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
